{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import training and holdout dataframes\n",
    "train_df = pd.read_csv('kc_house_data_train.csv')\n",
    "hold_df = pd.read_csv('kc_house_data_test_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'id', 'date', 'price', 'bedrooms', 'bathrooms',\n",
      "       'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view', 'condition',\n",
      "       'grade', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated',\n",
      "       'zipcode', 'lat', 'long', 'sqft_living15', 'sqft_lot15'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Print column names\n",
    "print(train_df.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create house ages\n",
    "def set_ages(df):\n",
    "    df['age'] = 2015 - df['yr_built']\n",
    "    return df\n",
    "\n",
    "# Create house ages for dataframes\n",
    "# train_df = set_age(train_df)\n",
    "# hold_df = set_age(hold_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transform year renovated\n",
    "def trans_ren(df):\n",
    "    df['yr_renovated'] = df['yr_renovated'] - 1933\n",
    "    return df\n",
    "\n",
    "# Transorm year renovated for dataframes\n",
    "# train_df = trans_ren(train_df)\n",
    "# hold_df = trans_ren(hold_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for creating grade dummy variables\n",
    "def set_grades(df):\n",
    "    dummies = pd.get_dummies(df['grade'], prefix='grade', drop_first=True)\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "    for grade in dummies.keys():\n",
    "        df[grade] = df[grade] * df['sqft_living']\n",
    "    return df\n",
    "\n",
    "# Create grade dummy variables for dataframes\n",
    "# train_df = set_grades(train_df)\n",
    "# hold_df = set_grades(hold_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for creating condition dummy variables\n",
    "def set_conditions(df):\n",
    "    dummies = pd.get_dummies(df['condition'], prefix='condition', drop_first=True)\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "    for condition in dummies.keys():\n",
    "        df[condition] = df[condition] * df['sqft_living']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for creating year dummy variables\n",
    "def set_years(df):\n",
    "    df['year'] = df['date'].apply(lambda x: int(x[0:4]))\n",
    "    dummies = pd.get_dummies(df['year'], drop_first=True)\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "    for year in dummies.keys():\n",
    "        df[year] = df[year] * df['sqft_living']\n",
    "    df.drop('year', axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "# Create year dummy variables for dataframes\n",
    "# train_df = set_years(train_df)\n",
    "# hold_df = set_years(hold_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for creating month dummy variables\n",
    "def set_months(df):\n",
    "#     months = {1:'jan', 2:'feb', 3:'mar', 4:'apr', 5:'may', 6:'jun',\n",
    "#               7:'jul', 8:'aug', 9:'sep', 10:'oct', 11:'nov', 12:'dec'}\n",
    "    df['month'] = df['date'].apply(lambda x: int(x[4:6]))\n",
    "    dummies = pd.get_dummies(df['month'], drop_first=True)\n",
    "#     for key in dummies.keys():\n",
    "#         if key in months.keys():\n",
    "#             dummies.rename({key : months[key]}, axis=1, inplace=True)\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "    for month in dummies.keys():\n",
    "        df[month] = df[month] * df['sqft_living']\n",
    "    df.drop('month', axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "# Create month dummy variables for dataframes\n",
    "# train_df = set_months(train_df)\n",
    "# hold_df = set_months(hold_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for creating zipcode dummy variables\n",
    "def set_zipcodes(df):\n",
    "    dummies = pd.get_dummies(df['zipcode'], drop_first=True)\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "    for zipcode in dummies.keys():\n",
    "        df[zipcode] = df[zipcode] * df['sqft_living']\n",
    "    return df\n",
    "\n",
    "# Create zipcode dummy variables for dataframes\n",
    "# train_df = set_zipcodes(train_df)\n",
    "# hold_df = set_zipcodes(hold_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transform waterfront variable\n",
    "def trans_waterfront(df):\n",
    "    df['waterfront'] = df['waterfront'] * df['sqft_living']\n",
    "    return df\n",
    "\n",
    "# Transform waterfront variable for dataframes\n",
    "# train_df = trans_waterfront(train_df)\n",
    "# hold_df = trans_waterfront(hold_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Features to Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create all features for dataframes\n",
    "def create_featuers(df):\n",
    "    df = set_ages(df)\n",
    "    df = trans_ren(df)\n",
    "    df = set_years(df)\n",
    "    df = set_months(df)\n",
    "    df = set_zipcodes(df)\n",
    "    df = trans_waterfront(df)\n",
    "    df = set_grades(df)\n",
    "    df = set_conditions(df)\n",
    "    return df\n",
    "\n",
    "# Create all features for dataframes\n",
    "train_df = create_featuers(train_df)\n",
    "hold_df = create_featuers(hold_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transform extreme values\n",
    "def fix_outliers(df, cols):\n",
    "    for col in cols:\n",
    "        if df[col].eq(0).any():\n",
    "            filt = df[col] == 0\n",
    "            df[f'{col}_nan'] = np.where(filt, np.nan, df[col])\n",
    "            std = df[f'{col}_nan'].std()\n",
    "            mean = df[f'{col}_nan'].mean()\n",
    "            del df[f'{col}_nan']\n",
    "        else:\n",
    "            std = df[col].std()\n",
    "            mean = df[col].mean()\n",
    "        value = mean+(8*std)\n",
    "        df[col] = df[col].apply(lambda x: value if (x>value) else x)\n",
    "    return df\n",
    "\n",
    "# Transform extreme values for specified columns dataframes\n",
    "outlier_cols = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'sqft_above',\n",
    "                'sqft_basement', 'sqft_living15', 'sqft_lot15']\n",
    "\n",
    "train_df = fix_outliers(train_df, outlier_cols)\n",
    "hold_df = fix_outliers(hold_df, outlier_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Function to create feature pplots\n",
    "# def pplot_features(df, features):\n",
    "#     pp_rows = [features[i:i+4] for i in range(0, len(features), 4)]\n",
    "#     for row in pp_rows:\n",
    "#         pp = sns.pairplot(data=df, y_vars=['price'], x_vars=row, kind='reg', height=3)\n",
    "#     return None\n",
    "\n",
    "# # Create feature pplots for training df\n",
    "# pplot_features(train_df, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data Into Train, Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tt_split(df):\n",
    "    ignore = ['Unnamed: 0', 'id', 'price', 'date', 'yr_built', 'zipcode',\n",
    "          'lat', 'long', 'sqft_living', 'sqft_above','grade', 'condition']\n",
    "    Y = df['price']\n",
    "    for col in ignore:\n",
    "        df = df.drop(columns=col, axis=1)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df, Y, random_state=22,test_size=0.2)\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "x_train, x_test, y_train, y_test = tt_split(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_hold = hold_df.drop(columns=['Unnamed: 0', 'id', 'date', 'yr_built', 'zipcode',\n",
    "          'lat', 'long', 'sqft_living', 'sqft_above','grade', 'condition'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th>  <td>   0.892</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.892</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   1064.</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 04 May 2020</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:57:44</td>     <th>  Log-Likelihood:    </th> <td>-1.8173e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 13832</td>      <th>  AIC:               </th>  <td>3.637e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 13724</td>      <th>  BIC:               </th>  <td>3.645e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>   107</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td></td>           <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>         <td> 2.091e+05</td> <td> 1.04e+04</td> <td>   20.151</td> <td> 0.000</td> <td> 1.89e+05</td> <td> 2.29e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bedrooms</th>      <td>-9880.4666</td> <td> 1557.358</td> <td>   -6.344</td> <td> 0.000</td> <td>-1.29e+04</td> <td>-6827.832</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bathrooms</th>     <td> 2.077e+04</td> <td> 2542.288</td> <td>    8.171</td> <td> 0.000</td> <td> 1.58e+04</td> <td> 2.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_lot</th>      <td>    0.5416</td> <td>    0.055</td> <td>    9.909</td> <td> 0.000</td> <td>    0.434</td> <td>    0.649</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>floors</th>        <td>-3.401e+04</td> <td> 2994.874</td> <td>  -11.357</td> <td> 0.000</td> <td>-3.99e+04</td> <td>-2.81e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>waterfront</th>    <td>  218.1411</td> <td>    3.617</td> <td>   60.310</td> <td> 0.000</td> <td>  211.051</td> <td>  225.231</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>view</th>          <td> 4.314e+04</td> <td> 1676.618</td> <td>   25.730</td> <td> 0.000</td> <td> 3.99e+04</td> <td> 4.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_basement</th> <td>  -75.9596</td> <td>    3.605</td> <td>  -21.071</td> <td> 0.000</td> <td>  -83.026</td> <td>  -68.893</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>yr_renovated</th>  <td>   23.3993</td> <td>    2.870</td> <td>    8.154</td> <td> 0.000</td> <td>   17.774</td> <td>   29.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_living15</th> <td>   57.7246</td> <td>    2.812</td> <td>   20.525</td> <td> 0.000</td> <td>   52.212</td> <td>   63.237</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_lot15</th>    <td>   -0.1232</td> <td>    0.074</td> <td>   -1.659</td> <td> 0.097</td> <td>   -0.269</td> <td>    0.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age</th>           <td>   21.1152</td> <td>   61.512</td> <td>    0.343</td> <td> 0.731</td> <td>  -99.457</td> <td>  141.688</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2015</th>          <td>   34.6510</td> <td>    3.190</td> <td>   10.862</td> <td> 0.000</td> <td>   28.398</td> <td>   40.904</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2</th>             <td>    0.3339</td> <td>    2.973</td> <td>    0.112</td> <td> 0.911</td> <td>   -5.493</td> <td>    6.161</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3</th>             <td>   11.0906</td> <td>    2.772</td> <td>    4.002</td> <td> 0.000</td> <td>    5.658</td> <td>   16.523</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4</th>             <td>   16.9240</td> <td>    2.651</td> <td>    6.384</td> <td> 0.000</td> <td>   11.728</td> <td>   22.120</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5</th>             <td>   26.6885</td> <td>    3.552</td> <td>    7.515</td> <td> 0.000</td> <td>   19.727</td> <td>   33.650</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6</th>             <td>   32.6463</td> <td>    4.133</td> <td>    7.899</td> <td> 0.000</td> <td>   24.545</td> <td>   40.748</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7</th>             <td>   32.5404</td> <td>    4.139</td> <td>    7.862</td> <td> 0.000</td> <td>   24.427</td> <td>   40.653</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8</th>             <td>   33.9690</td> <td>    4.180</td> <td>    8.126</td> <td> 0.000</td> <td>   25.775</td> <td>   42.163</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9</th>             <td>   30.4377</td> <td>    4.216</td> <td>    7.220</td> <td> 0.000</td> <td>   22.175</td> <td>   38.701</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10</th>            <td>   32.8040</td> <td>    4.180</td> <td>    7.847</td> <td> 0.000</td> <td>   24.610</td> <td>   40.998</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11</th>            <td>   31.7663</td> <td>    4.289</td> <td>    7.407</td> <td> 0.000</td> <td>   23.360</td> <td>   40.173</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12</th>            <td>   33.4231</td> <td>    4.289</td> <td>    7.792</td> <td> 0.000</td> <td>   25.015</td> <td>   41.831</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98002</th>         <td>   -4.6554</td> <td>    7.480</td> <td>   -0.622</td> <td> 0.534</td> <td>  -19.316</td> <td>   10.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98003</th>         <td>   -5.3643</td> <td>    6.014</td> <td>   -0.892</td> <td> 0.372</td> <td>  -17.153</td> <td>    6.425</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98004</th>         <td>  302.3059</td> <td>    5.052</td> <td>   59.839</td> <td> 0.000</td> <td>  292.403</td> <td>  312.209</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98005</th>         <td>  128.6846</td> <td>    5.904</td> <td>   21.796</td> <td> 0.000</td> <td>  117.112</td> <td>  140.257</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98006</th>         <td>  119.2181</td> <td>    4.813</td> <td>   24.771</td> <td> 0.000</td> <td>  109.784</td> <td>  128.652</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98007</th>         <td>  123.6548</td> <td>    7.175</td> <td>   17.233</td> <td> 0.000</td> <td>  109.590</td> <td>  137.720</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98008</th>         <td>  122.2895</td> <td>    5.781</td> <td>   21.153</td> <td> 0.000</td> <td>  110.957</td> <td>  133.622</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98010</th>         <td>   32.0766</td> <td>    7.772</td> <td>    4.127</td> <td> 0.000</td> <td>   16.842</td> <td>   47.311</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98011</th>         <td>   66.5990</td> <td>    6.324</td> <td>   10.532</td> <td> 0.000</td> <td>   54.204</td> <td>   78.994</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98014</th>         <td>   53.0253</td> <td>    7.198</td> <td>    7.367</td> <td> 0.000</td> <td>   38.916</td> <td>   67.134</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98019</th>         <td>   43.9674</td> <td>    6.360</td> <td>    6.913</td> <td> 0.000</td> <td>   31.500</td> <td>   56.435</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98022</th>         <td>  -12.3718</td> <td>    6.781</td> <td>   -1.824</td> <td> 0.068</td> <td>  -25.664</td> <td>    0.920</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98023</th>         <td>  -14.1083</td> <td>    5.227</td> <td>   -2.699</td> <td> 0.007</td> <td>  -24.353</td> <td>   -3.863</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98024</th>         <td>   74.6127</td> <td>    7.919</td> <td>    9.421</td> <td> 0.000</td> <td>   59.090</td> <td>   90.136</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98027</th>         <td>   81.9504</td> <td>    5.001</td> <td>   16.386</td> <td> 0.000</td> <td>   72.147</td> <td>   91.754</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98028</th>         <td>   69.3039</td> <td>    5.848</td> <td>   11.852</td> <td> 0.000</td> <td>   57.842</td> <td>   80.766</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98029</th>         <td>  101.4009</td> <td>    5.457</td> <td>   18.582</td> <td> 0.000</td> <td>   90.704</td> <td>  112.098</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98030</th>         <td>    1.6152</td> <td>    6.303</td> <td>    0.256</td> <td> 0.798</td> <td>  -10.739</td> <td>   13.969</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98031</th>         <td>    4.2989</td> <td>    6.094</td> <td>    0.705</td> <td> 0.481</td> <td>   -7.646</td> <td>   16.244</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98032</th>         <td>   -4.5438</td> <td>    8.566</td> <td>   -0.530</td> <td> 0.596</td> <td>  -21.335</td> <td>   12.247</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98033</th>         <td>  174.6734</td> <td>    5.029</td> <td>   34.731</td> <td> 0.000</td> <td>  164.815</td> <td>  184.532</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98034</th>         <td>  107.6438</td> <td>    5.244</td> <td>   20.526</td> <td> 0.000</td> <td>   97.364</td> <td>  117.923</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98038</th>         <td>   18.0647</td> <td>    5.011</td> <td>    3.605</td> <td> 0.000</td> <td>    8.242</td> <td>   27.887</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98039</th>         <td>  380.1417</td> <td>    6.819</td> <td>   55.744</td> <td> 0.000</td> <td>  366.775</td> <td>  393.509</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98040</th>         <td>  192.4373</td> <td>    5.082</td> <td>   37.867</td> <td> 0.000</td> <td>  182.476</td> <td>  202.399</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98042</th>         <td>    0.2350</td> <td>    5.217</td> <td>    0.045</td> <td> 0.964</td> <td>   -9.991</td> <td>   10.461</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98045</th>         <td>   40.8625</td> <td>    6.215</td> <td>    6.575</td> <td> 0.000</td> <td>   28.681</td> <td>   53.044</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98052</th>         <td>  112.9759</td> <td>    4.893</td> <td>   23.091</td> <td> 0.000</td> <td>  103.386</td> <td>  122.566</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98053</th>         <td>   83.3627</td> <td>    4.943</td> <td>   16.864</td> <td> 0.000</td> <td>   73.673</td> <td>   93.052</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98055</th>         <td>   18.9073</td> <td>    6.424</td> <td>    2.943</td> <td> 0.003</td> <td>    6.316</td> <td>   31.499</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98056</th>         <td>   53.2485</td> <td>    5.412</td> <td>    9.839</td> <td> 0.000</td> <td>   42.641</td> <td>   63.856</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98058</th>         <td>   18.3422</td> <td>    5.312</td> <td>    3.453</td> <td> 0.001</td> <td>    7.929</td> <td>   28.755</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98059</th>         <td>   53.3154</td> <td>    5.032</td> <td>   10.595</td> <td> 0.000</td> <td>   43.451</td> <td>   63.179</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98065</th>         <td>   51.2506</td> <td>    5.232</td> <td>    9.795</td> <td> 0.000</td> <td>   40.995</td> <td>   61.506</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98070</th>         <td>   24.2554</td> <td>    8.452</td> <td>    2.870</td> <td> 0.004</td> <td>    7.688</td> <td>   40.822</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98072</th>         <td>   78.1649</td> <td>    5.611</td> <td>   13.931</td> <td> 0.000</td> <td>   67.167</td> <td>   89.163</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98074</th>         <td>   86.9984</td> <td>    4.934</td> <td>   17.632</td> <td> 0.000</td> <td>   77.327</td> <td>   96.670</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98075</th>         <td>   79.5406</td> <td>    4.995</td> <td>   15.924</td> <td> 0.000</td> <td>   69.750</td> <td>   89.332</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98077</th>         <td>   63.5597</td> <td>    5.482</td> <td>   11.594</td> <td> 0.000</td> <td>   52.814</td> <td>   74.305</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98092</th>         <td>   -8.7672</td> <td>    5.391</td> <td>   -1.626</td> <td> 0.104</td> <td>  -19.334</td> <td>    1.800</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98102</th>         <td>  282.5454</td> <td>    7.762</td> <td>   36.403</td> <td> 0.000</td> <td>  267.332</td> <td>  297.759</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98103</th>         <td>  203.3392</td> <td>    5.736</td> <td>   35.449</td> <td> 0.000</td> <td>  192.096</td> <td>  214.583</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98105</th>         <td>  235.3768</td> <td>    6.261</td> <td>   37.591</td> <td> 0.000</td> <td>  223.103</td> <td>  247.650</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98106</th>         <td>   64.4297</td> <td>    6.688</td> <td>    9.634</td> <td> 0.000</td> <td>   51.321</td> <td>   77.539</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98107</th>         <td>  202.9402</td> <td>    7.157</td> <td>   28.356</td> <td> 0.000</td> <td>  188.912</td> <td>  216.968</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98108</th>         <td>   68.9793</td> <td>    7.539</td> <td>    9.149</td> <td> 0.000</td> <td>   54.201</td> <td>   83.758</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98109</th>         <td>  267.0468</td> <td>    8.063</td> <td>   33.120</td> <td> 0.000</td> <td>  251.242</td> <td>  282.851</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98112</th>         <td>  281.4164</td> <td>    5.506</td> <td>   51.107</td> <td> 0.000</td> <td>  270.623</td> <td>  292.210</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98115</th>         <td>  183.8724</td> <td>    5.421</td> <td>   33.919</td> <td> 0.000</td> <td>  173.247</td> <td>  194.498</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98116</th>         <td>  163.1099</td> <td>    6.085</td> <td>   26.806</td> <td> 0.000</td> <td>  151.183</td> <td>  175.037</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98117</th>         <td>  186.4803</td> <td>    5.677</td> <td>   32.851</td> <td> 0.000</td> <td>  175.353</td> <td>  197.607</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98118</th>         <td>   93.6362</td> <td>    5.652</td> <td>   16.566</td> <td> 0.000</td> <td>   82.557</td> <td>  104.715</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98119</th>         <td>  275.8822</td> <td>    6.852</td> <td>   40.262</td> <td> 0.000</td> <td>  262.451</td> <td>  289.313</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98122</th>         <td>  193.6634</td> <td>    6.362</td> <td>   30.442</td> <td> 0.000</td> <td>  181.194</td> <td>  206.133</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98125</th>         <td>  107.5224</td> <td>    5.832</td> <td>   18.435</td> <td> 0.000</td> <td>   96.090</td> <td>  118.955</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98126</th>         <td>  113.9169</td> <td>    6.514</td> <td>   17.487</td> <td> 0.000</td> <td>  101.148</td> <td>  126.686</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98133</th>         <td>   85.2647</td> <td>    5.781</td> <td>   14.748</td> <td> 0.000</td> <td>   73.932</td> <td>   96.597</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98136</th>         <td>  146.1740</td> <td>    6.477</td> <td>   22.568</td> <td> 0.000</td> <td>  133.478</td> <td>  158.870</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98144</th>         <td>  169.6457</td> <td>    5.765</td> <td>   29.425</td> <td> 0.000</td> <td>  158.345</td> <td>  180.947</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98146</th>         <td>   58.7032</td> <td>    6.477</td> <td>    9.064</td> <td> 0.000</td> <td>   46.008</td> <td>   71.399</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98148</th>         <td>   22.4697</td> <td>   13.068</td> <td>    1.719</td> <td> 0.086</td> <td>   -3.145</td> <td>   48.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98155</th>         <td>   78.8048</td> <td>    5.551</td> <td>   14.197</td> <td> 0.000</td> <td>   67.924</td> <td>   89.685</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98166</th>         <td>   31.8496</td> <td>    5.825</td> <td>    5.468</td> <td> 0.000</td> <td>   20.432</td> <td>   43.267</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98168</th>         <td>   18.7657</td> <td>    7.212</td> <td>    2.602</td> <td> 0.009</td> <td>    4.630</td> <td>   32.902</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98177</th>         <td>  114.6489</td> <td>    5.704</td> <td>   20.100</td> <td> 0.000</td> <td>  103.469</td> <td>  125.829</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98178</th>         <td>   19.8009</td> <td>    6.486</td> <td>    3.053</td> <td> 0.002</td> <td>    7.087</td> <td>   32.515</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98188</th>         <td>   24.6349</td> <td>    8.088</td> <td>    3.046</td> <td> 0.002</td> <td>    8.782</td> <td>   40.488</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98198</th>         <td>    1.0681</td> <td>    6.410</td> <td>    0.167</td> <td> 0.868</td> <td>  -11.496</td> <td>   13.632</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98199</th>         <td>  196.7888</td> <td>    5.627</td> <td>   34.973</td> <td> 0.000</td> <td>  185.759</td> <td>  207.818</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_3</th>       <td> -312.0731</td> <td>  185.664</td> <td>   -1.681</td> <td> 0.093</td> <td> -676.000</td> <td>   51.854</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_4</th>       <td> -194.6654</td> <td>   45.776</td> <td>   -4.253</td> <td> 0.000</td> <td> -284.392</td> <td> -104.939</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_5</th>       <td> -155.0449</td> <td>   23.529</td> <td>   -6.590</td> <td> 0.000</td> <td> -201.164</td> <td> -108.926</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_6</th>       <td> -132.7131</td> <td>   21.568</td> <td>   -6.153</td> <td> 0.000</td> <td> -174.990</td> <td>  -90.437</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_7</th>       <td> -111.2018</td> <td>   21.372</td> <td>   -5.203</td> <td> 0.000</td> <td> -153.094</td> <td>  -69.309</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_8</th>       <td>  -91.6598</td> <td>   21.356</td> <td>   -4.292</td> <td> 0.000</td> <td> -133.521</td> <td>  -49.798</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_9</th>       <td>  -69.7602</td> <td>   21.338</td> <td>   -3.269</td> <td> 0.001</td> <td> -111.586</td> <td>  -27.935</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_10</th>      <td>  -45.3562</td> <td>   21.353</td> <td>   -2.124</td> <td> 0.034</td> <td>  -87.210</td> <td>   -3.502</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_11</th>      <td>  -21.5605</td> <td>   21.375</td> <td>   -1.009</td> <td> 0.313</td> <td>  -63.459</td> <td>   20.338</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_12</th>      <td>   17.7318</td> <td>   21.465</td> <td>    0.826</td> <td> 0.409</td> <td>  -24.342</td> <td>   59.805</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_13</th>      <td>  119.0242</td> <td>   22.227</td> <td>    5.355</td> <td> 0.000</td> <td>   75.457</td> <td>  162.592</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition_2</th>   <td>   86.6240</td> <td>   21.836</td> <td>    3.967</td> <td> 0.000</td> <td>   43.822</td> <td>  129.426</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition_3</th>   <td>   93.6340</td> <td>   20.354</td> <td>    4.600</td> <td> 0.000</td> <td>   53.738</td> <td>  133.530</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition_4</th>   <td>  105.6303</td> <td>   20.352</td> <td>    5.190</td> <td> 0.000</td> <td>   65.738</td> <td>  145.523</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition_5</th>   <td>  126.8352</td> <td>   20.397</td> <td>    6.218</td> <td> 0.000</td> <td>   86.854</td> <td>  166.816</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>5660.646</td> <th>  Durbin-Watson:     </th>  <td>   2.003</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>262906.400</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 1.228</td>  <th>  Prob(JB):          </th>  <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>24.216</td>  <th>  Cond. No.          </th>  <td>4.38e+05</td> \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 4.38e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.892\n",
       "Model:                            OLS   Adj. R-squared:                  0.892\n",
       "Method:                 Least Squares   F-statistic:                     1064.\n",
       "Date:                Mon, 04 May 2020   Prob (F-statistic):               0.00\n",
       "Time:                        11:57:44   Log-Likelihood:            -1.8173e+05\n",
       "No. Observations:               13832   AIC:                         3.637e+05\n",
       "Df Residuals:                   13724   BIC:                         3.645e+05\n",
       "Df Model:                         107                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=================================================================================\n",
       "                    coef    std err          t      P>|t|      [0.025      0.975]\n",
       "---------------------------------------------------------------------------------\n",
       "const          2.091e+05   1.04e+04     20.151      0.000    1.89e+05    2.29e+05\n",
       "bedrooms      -9880.4666   1557.358     -6.344      0.000   -1.29e+04   -6827.832\n",
       "bathrooms      2.077e+04   2542.288      8.171      0.000    1.58e+04    2.58e+04\n",
       "sqft_lot          0.5416      0.055      9.909      0.000       0.434       0.649\n",
       "floors        -3.401e+04   2994.874    -11.357      0.000   -3.99e+04   -2.81e+04\n",
       "waterfront      218.1411      3.617     60.310      0.000     211.051     225.231\n",
       "view           4.314e+04   1676.618     25.730      0.000    3.99e+04    4.64e+04\n",
       "sqft_basement   -75.9596      3.605    -21.071      0.000     -83.026     -68.893\n",
       "yr_renovated     23.3993      2.870      8.154      0.000      17.774      29.024\n",
       "sqft_living15    57.7246      2.812     20.525      0.000      52.212      63.237\n",
       "sqft_lot15       -0.1232      0.074     -1.659      0.097      -0.269       0.022\n",
       "age              21.1152     61.512      0.343      0.731     -99.457     141.688\n",
       "2015             34.6510      3.190     10.862      0.000      28.398      40.904\n",
       "2                 0.3339      2.973      0.112      0.911      -5.493       6.161\n",
       "3                11.0906      2.772      4.002      0.000       5.658      16.523\n",
       "4                16.9240      2.651      6.384      0.000      11.728      22.120\n",
       "5                26.6885      3.552      7.515      0.000      19.727      33.650\n",
       "6                32.6463      4.133      7.899      0.000      24.545      40.748\n",
       "7                32.5404      4.139      7.862      0.000      24.427      40.653\n",
       "8                33.9690      4.180      8.126      0.000      25.775      42.163\n",
       "9                30.4377      4.216      7.220      0.000      22.175      38.701\n",
       "10               32.8040      4.180      7.847      0.000      24.610      40.998\n",
       "11               31.7663      4.289      7.407      0.000      23.360      40.173\n",
       "12               33.4231      4.289      7.792      0.000      25.015      41.831\n",
       "98002            -4.6554      7.480     -0.622      0.534     -19.316      10.006\n",
       "98003            -5.3643      6.014     -0.892      0.372     -17.153       6.425\n",
       "98004           302.3059      5.052     59.839      0.000     292.403     312.209\n",
       "98005           128.6846      5.904     21.796      0.000     117.112     140.257\n",
       "98006           119.2181      4.813     24.771      0.000     109.784     128.652\n",
       "98007           123.6548      7.175     17.233      0.000     109.590     137.720\n",
       "98008           122.2895      5.781     21.153      0.000     110.957     133.622\n",
       "98010            32.0766      7.772      4.127      0.000      16.842      47.311\n",
       "98011            66.5990      6.324     10.532      0.000      54.204      78.994\n",
       "98014            53.0253      7.198      7.367      0.000      38.916      67.134\n",
       "98019            43.9674      6.360      6.913      0.000      31.500      56.435\n",
       "98022           -12.3718      6.781     -1.824      0.068     -25.664       0.920\n",
       "98023           -14.1083      5.227     -2.699      0.007     -24.353      -3.863\n",
       "98024            74.6127      7.919      9.421      0.000      59.090      90.136\n",
       "98027            81.9504      5.001     16.386      0.000      72.147      91.754\n",
       "98028            69.3039      5.848     11.852      0.000      57.842      80.766\n",
       "98029           101.4009      5.457     18.582      0.000      90.704     112.098\n",
       "98030             1.6152      6.303      0.256      0.798     -10.739      13.969\n",
       "98031             4.2989      6.094      0.705      0.481      -7.646      16.244\n",
       "98032            -4.5438      8.566     -0.530      0.596     -21.335      12.247\n",
       "98033           174.6734      5.029     34.731      0.000     164.815     184.532\n",
       "98034           107.6438      5.244     20.526      0.000      97.364     117.923\n",
       "98038            18.0647      5.011      3.605      0.000       8.242      27.887\n",
       "98039           380.1417      6.819     55.744      0.000     366.775     393.509\n",
       "98040           192.4373      5.082     37.867      0.000     182.476     202.399\n",
       "98042             0.2350      5.217      0.045      0.964      -9.991      10.461\n",
       "98045            40.8625      6.215      6.575      0.000      28.681      53.044\n",
       "98052           112.9759      4.893     23.091      0.000     103.386     122.566\n",
       "98053            83.3627      4.943     16.864      0.000      73.673      93.052\n",
       "98055            18.9073      6.424      2.943      0.003       6.316      31.499\n",
       "98056            53.2485      5.412      9.839      0.000      42.641      63.856\n",
       "98058            18.3422      5.312      3.453      0.001       7.929      28.755\n",
       "98059            53.3154      5.032     10.595      0.000      43.451      63.179\n",
       "98065            51.2506      5.232      9.795      0.000      40.995      61.506\n",
       "98070            24.2554      8.452      2.870      0.004       7.688      40.822\n",
       "98072            78.1649      5.611     13.931      0.000      67.167      89.163\n",
       "98074            86.9984      4.934     17.632      0.000      77.327      96.670\n",
       "98075            79.5406      4.995     15.924      0.000      69.750      89.332\n",
       "98077            63.5597      5.482     11.594      0.000      52.814      74.305\n",
       "98092            -8.7672      5.391     -1.626      0.104     -19.334       1.800\n",
       "98102           282.5454      7.762     36.403      0.000     267.332     297.759\n",
       "98103           203.3392      5.736     35.449      0.000     192.096     214.583\n",
       "98105           235.3768      6.261     37.591      0.000     223.103     247.650\n",
       "98106            64.4297      6.688      9.634      0.000      51.321      77.539\n",
       "98107           202.9402      7.157     28.356      0.000     188.912     216.968\n",
       "98108            68.9793      7.539      9.149      0.000      54.201      83.758\n",
       "98109           267.0468      8.063     33.120      0.000     251.242     282.851\n",
       "98112           281.4164      5.506     51.107      0.000     270.623     292.210\n",
       "98115           183.8724      5.421     33.919      0.000     173.247     194.498\n",
       "98116           163.1099      6.085     26.806      0.000     151.183     175.037\n",
       "98117           186.4803      5.677     32.851      0.000     175.353     197.607\n",
       "98118            93.6362      5.652     16.566      0.000      82.557     104.715\n",
       "98119           275.8822      6.852     40.262      0.000     262.451     289.313\n",
       "98122           193.6634      6.362     30.442      0.000     181.194     206.133\n",
       "98125           107.5224      5.832     18.435      0.000      96.090     118.955\n",
       "98126           113.9169      6.514     17.487      0.000     101.148     126.686\n",
       "98133            85.2647      5.781     14.748      0.000      73.932      96.597\n",
       "98136           146.1740      6.477     22.568      0.000     133.478     158.870\n",
       "98144           169.6457      5.765     29.425      0.000     158.345     180.947\n",
       "98146            58.7032      6.477      9.064      0.000      46.008      71.399\n",
       "98148            22.4697     13.068      1.719      0.086      -3.145      48.084\n",
       "98155            78.8048      5.551     14.197      0.000      67.924      89.685\n",
       "98166            31.8496      5.825      5.468      0.000      20.432      43.267\n",
       "98168            18.7657      7.212      2.602      0.009       4.630      32.902\n",
       "98177           114.6489      5.704     20.100      0.000     103.469     125.829\n",
       "98178            19.8009      6.486      3.053      0.002       7.087      32.515\n",
       "98188            24.6349      8.088      3.046      0.002       8.782      40.488\n",
       "98198             1.0681      6.410      0.167      0.868     -11.496      13.632\n",
       "98199           196.7888      5.627     34.973      0.000     185.759     207.818\n",
       "grade_3        -312.0731    185.664     -1.681      0.093    -676.000      51.854\n",
       "grade_4        -194.6654     45.776     -4.253      0.000    -284.392    -104.939\n",
       "grade_5        -155.0449     23.529     -6.590      0.000    -201.164    -108.926\n",
       "grade_6        -132.7131     21.568     -6.153      0.000    -174.990     -90.437\n",
       "grade_7        -111.2018     21.372     -5.203      0.000    -153.094     -69.309\n",
       "grade_8         -91.6598     21.356     -4.292      0.000    -133.521     -49.798\n",
       "grade_9         -69.7602     21.338     -3.269      0.001    -111.586     -27.935\n",
       "grade_10        -45.3562     21.353     -2.124      0.034     -87.210      -3.502\n",
       "grade_11        -21.5605     21.375     -1.009      0.313     -63.459      20.338\n",
       "grade_12         17.7318     21.465      0.826      0.409     -24.342      59.805\n",
       "grade_13        119.0242     22.227      5.355      0.000      75.457     162.592\n",
       "condition_2      86.6240     21.836      3.967      0.000      43.822     129.426\n",
       "condition_3      93.6340     20.354      4.600      0.000      53.738     133.530\n",
       "condition_4     105.6303     20.352      5.190      0.000      65.738     145.523\n",
       "condition_5     126.8352     20.397      6.218      0.000      86.854     166.816\n",
       "==============================================================================\n",
       "Omnibus:                     5660.646   Durbin-Watson:                   2.003\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           262906.400\n",
       "Skew:                           1.228   Prob(JB):                         0.00\n",
       "Kurtosis:                      24.216   Cond. No.                     4.38e+05\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 4.38e+05. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = sm.OLS(y_train, sm.add_constant(x_train[[key for key in x_train]])).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/blantj/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 104576679889657.22, tolerance: 194381627077.73962\n",
      "  positive)\n"
     ]
    }
   ],
   "source": [
    "lasso = Lasso(alpha=0.01, normalize=False)\n",
    "lasso.fit(x_train, y_train)\n",
    "\n",
    "y_train_pred = lasso.predict(x_train)\n",
    "y_pred = lasso.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error: 75673.14766397967\n",
      "Testing Error: 129709.4618667734\n"
     ]
    }
   ],
   "source": [
    "train_rmse = metrics.mean_absolute_error(y_train, y_train_pred)\n",
    "test_rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Training Error: '+ str(train_rmse) )\n",
    "print('Testing Error: '+ str(test_rmse) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>floors</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>grade_3</th>\n",
       "      <th>grade_4</th>\n",
       "      <th>grade_5</th>\n",
       "      <th>grade_6</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>grade_7</th>\n",
       "      <th>grade_8</th>\n",
       "      <th>grade_9</th>\n",
       "      <th>...</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>98105</th>\n",
       "      <th>98109</th>\n",
       "      <th>98119</th>\n",
       "      <th>98112</th>\n",
       "      <th>98102</th>\n",
       "      <th>98004</th>\n",
       "      <th>98039</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>view</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-34104.552036</td>\n",
       "      <td>-10025.174444</td>\n",
       "      <td>-268.95006</td>\n",
       "      <td>-154.262145</td>\n",
       "      <td>-112.715802</td>\n",
       "      <td>-89.58916</td>\n",
       "      <td>-76.088031</td>\n",
       "      <td>-68.023611</td>\n",
       "      <td>-48.479312</td>\n",
       "      <td>-26.610308</td>\n",
       "      <td>...</td>\n",
       "      <td>218.032761</td>\n",
       "      <td>234.174704</td>\n",
       "      <td>265.780238</td>\n",
       "      <td>274.706294</td>\n",
       "      <td>280.069725</td>\n",
       "      <td>281.292279</td>\n",
       "      <td>300.953774</td>\n",
       "      <td>378.869292</td>\n",
       "      <td>20661.204939</td>\n",
       "      <td>43149.076256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         floors      bedrooms    grade_3     grade_4     grade_5   grade_6  \\\n",
       "0 -34104.552036 -10025.174444 -268.95006 -154.262145 -112.715802 -89.58916   \n",
       "\n",
       "   sqft_basement    grade_7    grade_8    grade_9  ...  waterfront  \\\n",
       "0     -76.088031 -68.023611 -48.479312 -26.610308  ...  218.032761   \n",
       "\n",
       "        98105       98109       98119       98112       98102       98004  \\\n",
       "0  234.174704  265.780238  274.706294  280.069725  281.292279  300.953774   \n",
       "\n",
       "        98039     bathrooms          view  \n",
       "0  378.869292  20661.204939  43149.076256  \n",
       "\n",
       "[1 rows x 107 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_coef = pd.DataFrame(data=lasso.coef_).T\n",
    "lasso_coef.columns = x_train.columns\n",
    "lasso_coef = lasso_coef.T.sort_values(by=0).T\n",
    "\n",
    "lasso_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model Without Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all = train_df['price']\n",
    "ignore = ['Unnamed: 0', 'id', 'price', 'date', 'yr_built', 'zipcode',\n",
    "          'lat', 'long', 'sqft_living', 'sqft_above','grade', 'condition']\n",
    "x_all = train_df\n",
    "x_all = x_all.drop(columns=ignore, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th>  <td>   0.890</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.889</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   1294.</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 04 May 2020</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:58:18</td>     <th>  Log-Likelihood:    </th> <td>-2.2732e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 17290</td>      <th>  AIC:               </th>  <td>4.548e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 17182</td>      <th>  BIC:               </th>  <td>4.557e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>   107</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td></td>           <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>         <td> 2.151e+05</td> <td> 9305.171</td> <td>   23.116</td> <td> 0.000</td> <td> 1.97e+05</td> <td> 2.33e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bedrooms</th>      <td>-8823.6541</td> <td> 1411.189</td> <td>   -6.253</td> <td> 0.000</td> <td>-1.16e+04</td> <td>-6057.580</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bathrooms</th>     <td> 2.171e+04</td> <td> 2285.022</td> <td>    9.502</td> <td> 0.000</td> <td> 1.72e+04</td> <td> 2.62e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_lot</th>      <td>    0.5550</td> <td>    0.050</td> <td>   11.058</td> <td> 0.000</td> <td>    0.457</td> <td>    0.653</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>floors</th>        <td>-3.108e+04</td> <td> 2694.308</td> <td>  -11.534</td> <td> 0.000</td> <td>-3.64e+04</td> <td>-2.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>waterfront</th>    <td>  221.6211</td> <td>    3.337</td> <td>   66.415</td> <td> 0.000</td> <td>  215.080</td> <td>  228.162</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>view</th>          <td> 4.306e+04</td> <td> 1505.846</td> <td>   28.594</td> <td> 0.000</td> <td> 4.01e+04</td> <td>  4.6e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_basement</th> <td>  -70.3771</td> <td>    3.234</td> <td>  -21.762</td> <td> 0.000</td> <td>  -76.716</td> <td>  -64.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>yr_renovated</th>  <td>   25.8073</td> <td>    2.564</td> <td>   10.064</td> <td> 0.000</td> <td>   20.781</td> <td>   30.833</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_living15</th> <td>   56.8486</td> <td>    2.535</td> <td>   22.429</td> <td> 0.000</td> <td>   51.881</td> <td>   61.817</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_lot15</th>    <td>   -0.1333</td> <td>    0.068</td> <td>   -1.970</td> <td> 0.049</td> <td>   -0.266</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age</th>           <td>   18.0193</td> <td>   55.218</td> <td>    0.326</td> <td> 0.744</td> <td>  -90.214</td> <td>  126.253</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2015</th>          <td>   38.3260</td> <td>    2.866</td> <td>   13.372</td> <td> 0.000</td> <td>   32.708</td> <td>   43.944</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2</th>             <td>    0.8935</td> <td>    2.668</td> <td>    0.335</td> <td> 0.738</td> <td>   -4.335</td> <td>    6.122</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3</th>             <td>   10.5565</td> <td>    2.472</td> <td>    4.271</td> <td> 0.000</td> <td>    5.712</td> <td>   15.401</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4</th>             <td>   16.0659</td> <td>    2.370</td> <td>    6.779</td> <td> 0.000</td> <td>   11.421</td> <td>   20.711</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5</th>             <td>   29.1465</td> <td>    3.171</td> <td>    9.193</td> <td> 0.000</td> <td>   22.932</td> <td>   35.361</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6</th>             <td>   35.8768</td> <td>    3.705</td> <td>    9.682</td> <td> 0.000</td> <td>   28.614</td> <td>   43.140</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7</th>             <td>   34.3441</td> <td>    3.706</td> <td>    9.267</td> <td> 0.000</td> <td>   27.080</td> <td>   41.609</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8</th>             <td>   36.6509</td> <td>    3.749</td> <td>    9.777</td> <td> 0.000</td> <td>   29.303</td> <td>   43.999</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9</th>             <td>   33.9611</td> <td>    3.777</td> <td>    8.991</td> <td> 0.000</td> <td>   26.558</td> <td>   41.365</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10</th>            <td>   36.0328</td> <td>    3.749</td> <td>    9.611</td> <td> 0.000</td> <td>   28.684</td> <td>   43.381</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11</th>            <td>   34.9402</td> <td>    3.840</td> <td>    9.100</td> <td> 0.000</td> <td>   27.414</td> <td>   42.466</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12</th>            <td>   36.0054</td> <td>    3.830</td> <td>    9.400</td> <td> 0.000</td> <td>   28.498</td> <td>   43.513</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98002</th>         <td>   -5.4775</td> <td>    6.900</td> <td>   -0.794</td> <td> 0.427</td> <td>  -19.003</td> <td>    8.048</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98003</th>         <td>   -6.0701</td> <td>    5.439</td> <td>   -1.116</td> <td> 0.264</td> <td>  -16.731</td> <td>    4.590</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98004</th>         <td>  298.0649</td> <td>    4.523</td> <td>   65.893</td> <td> 0.000</td> <td>  289.198</td> <td>  306.931</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98005</th>         <td>  131.2284</td> <td>    5.284</td> <td>   24.834</td> <td> 0.000</td> <td>  120.871</td> <td>  141.586</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98006</th>         <td>  120.1118</td> <td>    4.297</td> <td>   27.951</td> <td> 0.000</td> <td>  111.689</td> <td>  128.535</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98007</th>         <td>  125.8151</td> <td>    6.351</td> <td>   19.811</td> <td> 0.000</td> <td>  113.367</td> <td>  138.263</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98008</th>         <td>  127.7733</td> <td>    5.210</td> <td>   24.526</td> <td> 0.000</td> <td>  117.562</td> <td>  137.985</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98010</th>         <td>   31.1511</td> <td>    7.191</td> <td>    4.332</td> <td> 0.000</td> <td>   17.057</td> <td>   45.246</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98011</th>         <td>   68.4330</td> <td>    5.553</td> <td>   12.323</td> <td> 0.000</td> <td>   57.548</td> <td>   79.318</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98014</th>         <td>   50.8088</td> <td>    6.500</td> <td>    7.817</td> <td> 0.000</td> <td>   38.068</td> <td>   63.550</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98019</th>         <td>   45.0830</td> <td>    5.791</td> <td>    7.785</td> <td> 0.000</td> <td>   33.733</td> <td>   56.433</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98022</th>         <td>  -13.0644</td> <td>    6.024</td> <td>   -2.169</td> <td> 0.030</td> <td>  -24.873</td> <td>   -1.256</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98023</th>         <td>  -13.5571</td> <td>    4.682</td> <td>   -2.896</td> <td> 0.004</td> <td>  -22.734</td> <td>   -4.380</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98024</th>         <td>   77.5932</td> <td>    7.017</td> <td>   11.057</td> <td> 0.000</td> <td>   63.838</td> <td>   91.348</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98027</th>         <td>   81.4839</td> <td>    4.513</td> <td>   18.056</td> <td> 0.000</td> <td>   72.638</td> <td>   90.329</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98028</th>         <td>   70.2147</td> <td>    5.253</td> <td>   13.366</td> <td> 0.000</td> <td>   59.918</td> <td>   80.512</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98029</th>         <td>  101.3500</td> <td>    4.936</td> <td>   20.533</td> <td> 0.000</td> <td>   91.675</td> <td>  111.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98030</th>         <td>    1.5561</td> <td>    5.642</td> <td>    0.276</td> <td> 0.783</td> <td>   -9.502</td> <td>   12.615</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98031</th>         <td>    4.1367</td> <td>    5.558</td> <td>    0.744</td> <td> 0.457</td> <td>   -6.757</td> <td>   15.031</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98032</th>         <td>   -4.0882</td> <td>    7.649</td> <td>   -0.535</td> <td> 0.593</td> <td>  -19.080</td> <td>   10.904</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98033</th>         <td>  170.6299</td> <td>    4.498</td> <td>   37.933</td> <td> 0.000</td> <td>  161.813</td> <td>  179.447</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98034</th>         <td>  106.3578</td> <td>    4.646</td> <td>   22.894</td> <td> 0.000</td> <td>   97.252</td> <td>  115.464</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98038</th>         <td>   18.2456</td> <td>    4.504</td> <td>    4.051</td> <td> 0.000</td> <td>    9.417</td> <td>   27.074</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98039</th>         <td>  369.8129</td> <td>    5.998</td> <td>   61.659</td> <td> 0.000</td> <td>  358.057</td> <td>  381.569</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98040</th>         <td>  189.0579</td> <td>    4.547</td> <td>   41.575</td> <td> 0.000</td> <td>  180.145</td> <td>  197.971</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98042</th>         <td>    0.1005</td> <td>    4.692</td> <td>    0.021</td> <td> 0.983</td> <td>   -9.096</td> <td>    9.297</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98045</th>         <td>   41.5474</td> <td>    5.667</td> <td>    7.331</td> <td> 0.000</td> <td>   30.438</td> <td>   52.656</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98052</th>         <td>  113.1204</td> <td>    4.364</td> <td>   25.924</td> <td> 0.000</td> <td>  104.567</td> <td>  121.673</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98053</th>         <td>   84.6626</td> <td>    4.449</td> <td>   19.031</td> <td> 0.000</td> <td>   75.943</td> <td>   93.383</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98055</th>         <td>   16.7725</td> <td>    5.806</td> <td>    2.889</td> <td> 0.004</td> <td>    5.393</td> <td>   28.152</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98056</th>         <td>   54.2121</td> <td>    4.817</td> <td>   11.254</td> <td> 0.000</td> <td>   44.770</td> <td>   63.654</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98058</th>         <td>   18.9112</td> <td>    4.694</td> <td>    4.029</td> <td> 0.000</td> <td>    9.710</td> <td>   28.112</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98059</th>         <td>   52.3289</td> <td>    4.482</td> <td>   11.675</td> <td> 0.000</td> <td>   43.544</td> <td>   61.114</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98065</th>         <td>   51.8236</td> <td>    4.716</td> <td>   10.989</td> <td> 0.000</td> <td>   42.580</td> <td>   61.068</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98070</th>         <td>   22.6809</td> <td>    7.297</td> <td>    3.108</td> <td> 0.002</td> <td>    8.379</td> <td>   36.983</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98072</th>         <td>   75.8650</td> <td>    5.026</td> <td>   15.095</td> <td> 0.000</td> <td>   66.014</td> <td>   85.716</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98074</th>         <td>   88.2851</td> <td>    4.420</td> <td>   19.974</td> <td> 0.000</td> <td>   79.621</td> <td>   96.949</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98075</th>         <td>   80.8571</td> <td>    4.452</td> <td>   18.164</td> <td> 0.000</td> <td>   72.131</td> <td>   89.583</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98077</th>         <td>   62.4806</td> <td>    4.946</td> <td>   12.632</td> <td> 0.000</td> <td>   52.785</td> <td>   72.176</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98092</th>         <td>   -7.2463</td> <td>    4.860</td> <td>   -1.491</td> <td> 0.136</td> <td>  -16.773</td> <td>    2.280</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98102</th>         <td>  255.4731</td> <td>    6.674</td> <td>   38.276</td> <td> 0.000</td> <td>  242.391</td> <td>  268.556</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98103</th>         <td>  197.8830</td> <td>    5.089</td> <td>   38.882</td> <td> 0.000</td> <td>  187.907</td> <td>  207.859</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98105</th>         <td>  235.9794</td> <td>    5.561</td> <td>   42.438</td> <td> 0.000</td> <td>  225.080</td> <td>  246.879</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98106</th>         <td>   63.2642</td> <td>    6.053</td> <td>   10.453</td> <td> 0.000</td> <td>   51.401</td> <td>   75.128</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98107</th>         <td>  206.7386</td> <td>    6.310</td> <td>   32.764</td> <td> 0.000</td> <td>  194.370</td> <td>  219.107</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98108</th>         <td>   67.4572</td> <td>    6.719</td> <td>   10.040</td> <td> 0.000</td> <td>   54.287</td> <td>   80.627</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98109</th>         <td>  267.3672</td> <td>    7.092</td> <td>   37.697</td> <td> 0.000</td> <td>  253.465</td> <td>  281.269</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98112</th>         <td>  286.5402</td> <td>    4.938</td> <td>   58.023</td> <td> 0.000</td> <td>  276.860</td> <td>  296.220</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98115</th>         <td>  180.9427</td> <td>    4.809</td> <td>   37.628</td> <td> 0.000</td> <td>  171.517</td> <td>  190.368</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98116</th>         <td>  161.3079</td> <td>    5.398</td> <td>   29.885</td> <td> 0.000</td> <td>  150.728</td> <td>  171.888</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98117</th>         <td>  184.1257</td> <td>    5.090</td> <td>   36.173</td> <td> 0.000</td> <td>  174.149</td> <td>  194.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98118</th>         <td>   88.9323</td> <td>    5.058</td> <td>   17.581</td> <td> 0.000</td> <td>   79.017</td> <td>   98.847</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98119</th>         <td>  272.5091</td> <td>    6.092</td> <td>   44.733</td> <td> 0.000</td> <td>  260.568</td> <td>  284.450</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98122</th>         <td>  190.1408</td> <td>    5.809</td> <td>   32.731</td> <td> 0.000</td> <td>  178.754</td> <td>  201.527</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98125</th>         <td>  107.7404</td> <td>    5.208</td> <td>   20.688</td> <td> 0.000</td> <td>   97.532</td> <td>  117.949</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98126</th>         <td>  113.0262</td> <td>    5.848</td> <td>   19.328</td> <td> 0.000</td> <td>  101.564</td> <td>  124.489</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98133</th>         <td>   82.0301</td> <td>    5.211</td> <td>   15.742</td> <td> 0.000</td> <td>   71.816</td> <td>   92.244</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98136</th>         <td>  144.7842</td> <td>    5.880</td> <td>   24.622</td> <td> 0.000</td> <td>  133.258</td> <td>  156.310</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98144</th>         <td>  168.0491</td> <td>    5.152</td> <td>   32.621</td> <td> 0.000</td> <td>  157.951</td> <td>  178.147</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98146</th>         <td>   56.4232</td> <td>    5.829</td> <td>    9.679</td> <td> 0.000</td> <td>   44.997</td> <td>   67.849</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98148</th>         <td>   20.3122</td> <td>   11.993</td> <td>    1.694</td> <td> 0.090</td> <td>   -3.196</td> <td>   43.820</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98155</th>         <td>   76.1012</td> <td>    5.076</td> <td>   14.993</td> <td> 0.000</td> <td>   66.152</td> <td>   86.050</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98166</th>         <td>   35.1159</td> <td>    5.290</td> <td>    6.639</td> <td> 0.000</td> <td>   24.748</td> <td>   45.484</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98168</th>         <td>   18.2371</td> <td>    6.553</td> <td>    2.783</td> <td> 0.005</td> <td>    5.392</td> <td>   31.082</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98177</th>         <td>  125.1630</td> <td>    5.148</td> <td>   24.315</td> <td> 0.000</td> <td>  115.073</td> <td>  135.253</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98178</th>         <td>   20.4087</td> <td>    5.875</td> <td>    3.474</td> <td> 0.001</td> <td>    8.893</td> <td>   31.924</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98188</th>         <td>   21.3754</td> <td>    7.209</td> <td>    2.965</td> <td> 0.003</td> <td>    7.246</td> <td>   35.505</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98198</th>         <td>    0.4694</td> <td>    5.805</td> <td>    0.081</td> <td> 0.936</td> <td>  -10.908</td> <td>   11.847</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>98199</th>         <td>  194.6572</td> <td>    4.986</td> <td>   39.042</td> <td> 0.000</td> <td>  184.885</td> <td>  204.430</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_3</th>       <td> -201.4638</td> <td>  148.125</td> <td>   -1.360</td> <td> 0.174</td> <td> -491.805</td> <td>   88.877</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_4</th>       <td> -186.2602</td> <td>   41.875</td> <td>   -4.448</td> <td> 0.000</td> <td> -268.340</td> <td> -104.180</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_5</th>       <td> -136.8018</td> <td>   20.554</td> <td>   -6.656</td> <td> 0.000</td> <td> -177.089</td> <td>  -96.515</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_6</th>       <td> -111.7207</td> <td>   19.062</td> <td>   -5.861</td> <td> 0.000</td> <td> -149.085</td> <td>  -74.357</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_7</th>       <td>  -88.8832</td> <td>   18.936</td> <td>   -4.694</td> <td> 0.000</td> <td> -126.000</td> <td>  -51.766</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_8</th>       <td>  -69.6136</td> <td>   18.922</td> <td>   -3.679</td> <td> 0.000</td> <td> -106.702</td> <td>  -32.525</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_9</th>       <td>  -46.2774</td> <td>   18.911</td> <td>   -2.447</td> <td> 0.014</td> <td>  -83.346</td> <td>   -9.209</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_10</th>      <td>  -20.0867</td> <td>   18.924</td> <td>   -1.061</td> <td> 0.289</td> <td>  -57.180</td> <td>   17.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_11</th>      <td>    4.2024</td> <td>   18.947</td> <td>    0.222</td> <td> 0.824</td> <td>  -32.936</td> <td>   41.341</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_12</th>      <td>   42.8102</td> <td>   19.047</td> <td>    2.248</td> <td> 0.025</td> <td>    5.476</td> <td>   80.144</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_13</th>      <td>  139.6038</td> <td>   19.548</td> <td>    7.142</td> <td> 0.000</td> <td>  101.288</td> <td>  177.919</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition_2</th>   <td>   44.7837</td> <td>   19.197</td> <td>    2.333</td> <td> 0.020</td> <td>    7.156</td> <td>   82.412</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition_3</th>   <td>   62.7008</td> <td>   18.042</td> <td>    3.475</td> <td> 0.001</td> <td>   27.337</td> <td>   98.065</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition_4</th>   <td>   74.8195</td> <td>   18.041</td> <td>    4.147</td> <td> 0.000</td> <td>   39.457</td> <td>  110.182</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition_5</th>   <td>   97.5727</td> <td>   18.083</td> <td>    5.396</td> <td> 0.000</td> <td>   62.128</td> <td>  133.017</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>7326.552</td> <th>  Durbin-Watson:     </th>  <td>   1.996</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>332440.706</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 1.308</td>  <th>  Prob(JB):          </th>  <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>24.322</td>  <th>  Cond. No.          </th>  <td>4.32e+05</td> \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 4.32e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.890\n",
       "Model:                            OLS   Adj. R-squared:                  0.889\n",
       "Method:                 Least Squares   F-statistic:                     1294.\n",
       "Date:                Mon, 04 May 2020   Prob (F-statistic):               0.00\n",
       "Time:                        11:58:18   Log-Likelihood:            -2.2732e+05\n",
       "No. Observations:               17290   AIC:                         4.548e+05\n",
       "Df Residuals:                   17182   BIC:                         4.557e+05\n",
       "Df Model:                         107                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=================================================================================\n",
       "                    coef    std err          t      P>|t|      [0.025      0.975]\n",
       "---------------------------------------------------------------------------------\n",
       "const          2.151e+05   9305.171     23.116      0.000    1.97e+05    2.33e+05\n",
       "bedrooms      -8823.6541   1411.189     -6.253      0.000   -1.16e+04   -6057.580\n",
       "bathrooms      2.171e+04   2285.022      9.502      0.000    1.72e+04    2.62e+04\n",
       "sqft_lot          0.5550      0.050     11.058      0.000       0.457       0.653\n",
       "floors        -3.108e+04   2694.308    -11.534      0.000   -3.64e+04   -2.58e+04\n",
       "waterfront      221.6211      3.337     66.415      0.000     215.080     228.162\n",
       "view           4.306e+04   1505.846     28.594      0.000    4.01e+04     4.6e+04\n",
       "sqft_basement   -70.3771      3.234    -21.762      0.000     -76.716     -64.038\n",
       "yr_renovated     25.8073      2.564     10.064      0.000      20.781      30.833\n",
       "sqft_living15    56.8486      2.535     22.429      0.000      51.881      61.817\n",
       "sqft_lot15       -0.1333      0.068     -1.970      0.049      -0.266      -0.001\n",
       "age              18.0193     55.218      0.326      0.744     -90.214     126.253\n",
       "2015             38.3260      2.866     13.372      0.000      32.708      43.944\n",
       "2                 0.8935      2.668      0.335      0.738      -4.335       6.122\n",
       "3                10.5565      2.472      4.271      0.000       5.712      15.401\n",
       "4                16.0659      2.370      6.779      0.000      11.421      20.711\n",
       "5                29.1465      3.171      9.193      0.000      22.932      35.361\n",
       "6                35.8768      3.705      9.682      0.000      28.614      43.140\n",
       "7                34.3441      3.706      9.267      0.000      27.080      41.609\n",
       "8                36.6509      3.749      9.777      0.000      29.303      43.999\n",
       "9                33.9611      3.777      8.991      0.000      26.558      41.365\n",
       "10               36.0328      3.749      9.611      0.000      28.684      43.381\n",
       "11               34.9402      3.840      9.100      0.000      27.414      42.466\n",
       "12               36.0054      3.830      9.400      0.000      28.498      43.513\n",
       "98002            -5.4775      6.900     -0.794      0.427     -19.003       8.048\n",
       "98003            -6.0701      5.439     -1.116      0.264     -16.731       4.590\n",
       "98004           298.0649      4.523     65.893      0.000     289.198     306.931\n",
       "98005           131.2284      5.284     24.834      0.000     120.871     141.586\n",
       "98006           120.1118      4.297     27.951      0.000     111.689     128.535\n",
       "98007           125.8151      6.351     19.811      0.000     113.367     138.263\n",
       "98008           127.7733      5.210     24.526      0.000     117.562     137.985\n",
       "98010            31.1511      7.191      4.332      0.000      17.057      45.246\n",
       "98011            68.4330      5.553     12.323      0.000      57.548      79.318\n",
       "98014            50.8088      6.500      7.817      0.000      38.068      63.550\n",
       "98019            45.0830      5.791      7.785      0.000      33.733      56.433\n",
       "98022           -13.0644      6.024     -2.169      0.030     -24.873      -1.256\n",
       "98023           -13.5571      4.682     -2.896      0.004     -22.734      -4.380\n",
       "98024            77.5932      7.017     11.057      0.000      63.838      91.348\n",
       "98027            81.4839      4.513     18.056      0.000      72.638      90.329\n",
       "98028            70.2147      5.253     13.366      0.000      59.918      80.512\n",
       "98029           101.3500      4.936     20.533      0.000      91.675     111.025\n",
       "98030             1.5561      5.642      0.276      0.783      -9.502      12.615\n",
       "98031             4.1367      5.558      0.744      0.457      -6.757      15.031\n",
       "98032            -4.0882      7.649     -0.535      0.593     -19.080      10.904\n",
       "98033           170.6299      4.498     37.933      0.000     161.813     179.447\n",
       "98034           106.3578      4.646     22.894      0.000      97.252     115.464\n",
       "98038            18.2456      4.504      4.051      0.000       9.417      27.074\n",
       "98039           369.8129      5.998     61.659      0.000     358.057     381.569\n",
       "98040           189.0579      4.547     41.575      0.000     180.145     197.971\n",
       "98042             0.1005      4.692      0.021      0.983      -9.096       9.297\n",
       "98045            41.5474      5.667      7.331      0.000      30.438      52.656\n",
       "98052           113.1204      4.364     25.924      0.000     104.567     121.673\n",
       "98053            84.6626      4.449     19.031      0.000      75.943      93.383\n",
       "98055            16.7725      5.806      2.889      0.004       5.393      28.152\n",
       "98056            54.2121      4.817     11.254      0.000      44.770      63.654\n",
       "98058            18.9112      4.694      4.029      0.000       9.710      28.112\n",
       "98059            52.3289      4.482     11.675      0.000      43.544      61.114\n",
       "98065            51.8236      4.716     10.989      0.000      42.580      61.068\n",
       "98070            22.6809      7.297      3.108      0.002       8.379      36.983\n",
       "98072            75.8650      5.026     15.095      0.000      66.014      85.716\n",
       "98074            88.2851      4.420     19.974      0.000      79.621      96.949\n",
       "98075            80.8571      4.452     18.164      0.000      72.131      89.583\n",
       "98077            62.4806      4.946     12.632      0.000      52.785      72.176\n",
       "98092            -7.2463      4.860     -1.491      0.136     -16.773       2.280\n",
       "98102           255.4731      6.674     38.276      0.000     242.391     268.556\n",
       "98103           197.8830      5.089     38.882      0.000     187.907     207.859\n",
       "98105           235.9794      5.561     42.438      0.000     225.080     246.879\n",
       "98106            63.2642      6.053     10.453      0.000      51.401      75.128\n",
       "98107           206.7386      6.310     32.764      0.000     194.370     219.107\n",
       "98108            67.4572      6.719     10.040      0.000      54.287      80.627\n",
       "98109           267.3672      7.092     37.697      0.000     253.465     281.269\n",
       "98112           286.5402      4.938     58.023      0.000     276.860     296.220\n",
       "98115           180.9427      4.809     37.628      0.000     171.517     190.368\n",
       "98116           161.3079      5.398     29.885      0.000     150.728     171.888\n",
       "98117           184.1257      5.090     36.173      0.000     174.149     194.103\n",
       "98118            88.9323      5.058     17.581      0.000      79.017      98.847\n",
       "98119           272.5091      6.092     44.733      0.000     260.568     284.450\n",
       "98122           190.1408      5.809     32.731      0.000     178.754     201.527\n",
       "98125           107.7404      5.208     20.688      0.000      97.532     117.949\n",
       "98126           113.0262      5.848     19.328      0.000     101.564     124.489\n",
       "98133            82.0301      5.211     15.742      0.000      71.816      92.244\n",
       "98136           144.7842      5.880     24.622      0.000     133.258     156.310\n",
       "98144           168.0491      5.152     32.621      0.000     157.951     178.147\n",
       "98146            56.4232      5.829      9.679      0.000      44.997      67.849\n",
       "98148            20.3122     11.993      1.694      0.090      -3.196      43.820\n",
       "98155            76.1012      5.076     14.993      0.000      66.152      86.050\n",
       "98166            35.1159      5.290      6.639      0.000      24.748      45.484\n",
       "98168            18.2371      6.553      2.783      0.005       5.392      31.082\n",
       "98177           125.1630      5.148     24.315      0.000     115.073     135.253\n",
       "98178            20.4087      5.875      3.474      0.001       8.893      31.924\n",
       "98188            21.3754      7.209      2.965      0.003       7.246      35.505\n",
       "98198             0.4694      5.805      0.081      0.936     -10.908      11.847\n",
       "98199           194.6572      4.986     39.042      0.000     184.885     204.430\n",
       "grade_3        -201.4638    148.125     -1.360      0.174    -491.805      88.877\n",
       "grade_4        -186.2602     41.875     -4.448      0.000    -268.340    -104.180\n",
       "grade_5        -136.8018     20.554     -6.656      0.000    -177.089     -96.515\n",
       "grade_6        -111.7207     19.062     -5.861      0.000    -149.085     -74.357\n",
       "grade_7         -88.8832     18.936     -4.694      0.000    -126.000     -51.766\n",
       "grade_8         -69.6136     18.922     -3.679      0.000    -106.702     -32.525\n",
       "grade_9         -46.2774     18.911     -2.447      0.014     -83.346      -9.209\n",
       "grade_10        -20.0867     18.924     -1.061      0.289     -57.180      17.007\n",
       "grade_11          4.2024     18.947      0.222      0.824     -32.936      41.341\n",
       "grade_12         42.8102     19.047      2.248      0.025       5.476      80.144\n",
       "grade_13        139.6038     19.548      7.142      0.000     101.288     177.919\n",
       "condition_2      44.7837     19.197      2.333      0.020       7.156      82.412\n",
       "condition_3      62.7008     18.042      3.475      0.001      27.337      98.065\n",
       "condition_4      74.8195     18.041      4.147      0.000      39.457     110.182\n",
       "condition_5      97.5727     18.083      5.396      0.000      62.128     133.017\n",
       "==============================================================================\n",
       "Omnibus:                     7326.552   Durbin-Watson:                   1.996\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           332440.706\n",
       "Skew:                           1.308   Prob(JB):                         0.00\n",
       "Kurtosis:                      24.322   Cond. No.                     4.32e+05\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 4.32e+05. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_all = sm.OLS(y_all, sm.add_constant(x_all[[key for key in x_all]])).fit()\n",
    "model_all.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/blantj/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 133003112601269.28, tolerance: 240951709851.4366\n",
      "  positive)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.01, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "      normalize=False, positive=False, precompute=False, random_state=None,\n",
       "      selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_all = Lasso(alpha=0.01, normalize=False)\n",
    "lasso_all.fit(x_all, y_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 107 is different from 106)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-4211ba576466>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_hold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlasso_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_hold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_hold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \"\"\"\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0m_preprocess_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstaticmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_preprocess_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py\u001b[0m in \u001b[0;36m_decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    792\u001b[0m                                    dense_output=True) + self.intercept_\n\u001b[1;32m    793\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36m_decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'coo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         return safe_sparse_dot(X, self.coef_.T,\n\u001b[0;32m--> 209\u001b[0;31m                                dense_output=True) + self.intercept_\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 107 is different from 106)"
     ]
    }
   ],
   "source": [
    "y_hold = lasso_all.predict(x_hold)\n",
    "y_hold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_all.keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes (TESTING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Recreate original dataframe without any added columns\n",
    "# TESTING = pd.read_csv('kc_house_data_train.csv')\n",
    "\n",
    "# # Create features list for original dataframe\n",
    "# TESTING_FEATURES = set_features(TESTING)\n",
    "\n",
    "# # Run model on original dataframe\n",
    "# set_model(TESTING, TESTING_FEATURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unused Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to create target variable\n",
    "# def set_target(df):\n",
    "#     target = df['price']\n",
    "#     return target\n",
    "\n",
    "# # Create target variable for dataframes\n",
    "# train_target = set_target(train_df)\n",
    "# test_target = set_target(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function for creating grade dummy variables\n",
    "# def set_grades(df):\n",
    "#     dummies = pd.get_dummies(df['grade'], prefix='grade', drop_first=True)\n",
    "#     df = pd.concat([df, dummies], axis=1)\n",
    "#     return df\n",
    "\n",
    "# # Create grade dummy variables for dataframes\n",
    "# train_df = set_grades(train_df)\n",
    "# holdout_df = set_grades(holdout_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to transform bedrooms outliers\n",
    "# def bedrooms_trans(df):\n",
    "#     filt = df['bedrooms'] < 10\n",
    "#     df['bedrooms'] = np.where(filt, df['bedrooms'], 10)\n",
    "#     return df\n",
    "\n",
    "# # Transform bedrooms outliers in dataframes \n",
    "# train_df = bedrooms_trans(train_df)\n",
    "# hold_df = bedrooms_trans(hold_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to transform bathrooms outliers\n",
    "# def bathrooms_trans(df):\n",
    "#     low_filt = df['bathrooms'] > .5\n",
    "#     df['bathrooms'] = np.where(low_filt, df['bathrooms'], .5)\n",
    "#     high_filt = df['bathrooms'] < 5\n",
    "#     df['bathrooms'] = np.where(high_filt, df['bathrooms'], 5)\n",
    "#     return df\n",
    "\n",
    "# # Transform bedrooms outliers in dataframes \n",
    "# train_df = bathrooms_trans(train_df)\n",
    "# hold_df = bathrooms_trans(hold_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to create regression modeln (smf method)\n",
    "# def set_model(df, features):\n",
    "#     formula = 'price~' + '+'.join([f'{ft}' for ft in features])\n",
    "#     model = smf.ols(formula=formula, data=df).fit()\n",
    "#     return model.summary()\n",
    "\n",
    "# # Run model on train dataframe\n",
    "# set_model(train_df, train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to create age range dummy variables\n",
    "# def set_ages(df):\n",
    "#     year = df['yr_built']\n",
    "#     # df['age_new'] = np.where(age==2015, 1, 0)\n",
    "#     df['age_10'] = np.where((year>2004) & (year<2015), 1, 0)\n",
    "#     df['age_20'] = np.where((year>1994) & (year<2005), 1, 0)\n",
    "#     df['age_30'] = np.where((year>1984) & (year<1995), 1, 0)\n",
    "#     df['age_40'] = np.where((year>1974) & (year<1985), 1, 0)\n",
    "#     df['age_50'] = np.where((year>1964) & (year<1975), 1, 0)\n",
    "#     df['age_60'] = np.where((year>1954) & (year<1965), 1, 0)\n",
    "#     df['age_70'] = np.where((year>1944) & (year<1955), 1, 0)\n",
    "#     df['age_80'] = np.where((year>1934) & (year<1945), 1, 0)\n",
    "#     df['age_90'] = np.where((year>1924) & (year<1935), 1, 0)\n",
    "#     df['age_100'] = np.where((year>1914) & (year<1925), 1, 0)\n",
    "#     df['age_old'] = np.where((year<1915), 1, 0)\n",
    "#     return df\n",
    "\n",
    "# # Create age range dummy variables for dataframes\n",
    "# # train_df = set_ages(train_df)\n",
    "# # hold_df = set_ages(hold_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to create renovation age range dummy variables\n",
    "# def set_ren_ages(df):\n",
    "#     year = df['yr_built']\n",
    "#     ren_yr = df['yr_renovated']\n",
    "#     df['not_ren'] = np.where((year!=2015) & (ren_yr==0), 1, 0)\n",
    "#     df['ren_5'] = np.where((ren_yr>2010), 1, 0)\n",
    "#     df['ren_10'] = np.where((ren_yr>2005) & (ren_yr<2011), 1, 0)\n",
    "#     df['ren_15'] = np.where((ren_yr>2000) & (ren_yr<2006), 1, 0)\n",
    "#     df['ren_20'] = np.where((ren_yr>1995) & (ren_yr<2001), 1, 0)\n",
    "#     df['ren_25'] = np.where((ren_yr>1990) & (ren_yr<1996), 1, 0)\n",
    "#     df['ren_30'] = np.where((ren_yr>1985) & (ren_yr<1991), 1, 0)\n",
    "#     # df['ren_old'] = np.where((ren_yr>0) & (ren_yr<1986), 1, 0)\n",
    "#     return df\n",
    "\n",
    "# # Create renovation age range dummy variables for dataframes\n",
    "# # train_df = set_ren_ages(train_df)\n",
    "# # hold_df = set_ren_ages(hold_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Transform grade variable\n",
    "# df_test = train_df\n",
    "# df_test['grade_log'] = df_test['grade'].apply(lambda x: math.log(x))\n",
    "\n",
    "# df_test = train_df\n",
    "# df_test['grade_exp'] = df_test['grade'].apply(lambda x: math.exp(x))\n",
    "\n",
    "# df_test = train_df\n",
    "# df_test['price_log'] = df_test['price'].apply(lambda x: math.log(x))\n",
    "\n",
    "\n",
    "# test1 = df_test.groupby('grade_exp')['price'].mean()\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.scatter(test1.index, test1.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to create features list\n",
    "# def set_features(df):\n",
    "#     ignore = ['Unnamed: 0', 'id', 'price', 'date', 'yr_built', 'zipcode',\n",
    "#               'lat', 'long', 'sqft_living', 'sqft_above']\n",
    "#     features = list(df.keys())\n",
    "#     for feature in ignore:\n",
    "#         if feature in features:\n",
    "#             features.remove(feature)\n",
    "#     return features\n",
    "\n",
    "# # Create features list for dataframes\n",
    "# features = set_features(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to create regression model\n",
    "# def set_model(df, features):\n",
    "#     model = sm.OLS(y_train, sm.add_constant(\n",
    "#         df[[key for key in features]])).fit()\n",
    "#     return model.summary()\n",
    "# # Run model on training dataframe\n",
    "# set_model(train_df, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split into x dataframes and y series for train and test data\n",
    "# Y = train_df['price']\n",
    "# X = train_df.drop(['price'], axis=1)\n",
    "# x_train, x_test, y_train, y_test = train_test_split(\n",
    "#     X, Y, random_state=22,test_size=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
